{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f38c17c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import esm\n",
    "import scipy\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from numpy import asarray\n",
    "from numpy import savez_compressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "850d5a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PDB_csv_path = \"/home/PC/Desktop/Protein_language_model/embeddings/PDB_LIST.csv\"\n",
    "PDB_name_df = pd.read_csv(PDB_csv_path)\n",
    "PDBS = PDB_name_df['PDB_ID'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2981885f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def exctracting_embeddings_esm2(pdb):\n",
    "    mutations2= [] \n",
    "    Xs2 = []\n",
    "    for header2, _seq2 in esm.data.read_fasta(FASTA_PATH2):\n",
    "        scaled_effect2 = header2.split('|')[-1]\n",
    "        mutations2.append(scaled_effect2)\n",
    "        fn = f'{EMB_PATH2}/{header2[1:]}.pt'\n",
    "        embs2 = torch.load(fn)\n",
    "        Xs2.append(embs2['representations'][33])\n",
    "    Xs2 = torch.stack(Xs2, dim=0).numpy()\n",
    "    \n",
    "    return Xs2\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "def exctracting_embeddings_esm2_wild(pdb):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    ys = df['bind'].tolist()\n",
    "    no_of_mutations = len(ys)\n",
    "    mutations2_w= [] \n",
    "    Xs2_w = []\n",
    "    for header2, _seq2 in esm.data.read_fasta(FASTA_PATH2_w):\n",
    "        scaled_effect2_w = header2.split('|')[-1]\n",
    "        mutations2_w.append(scaled_effect2_w)\n",
    "        fn = f'{EMB_PATH2_w}/{header2[1:]}.pt'\n",
    "        embs2 = torch.load(fn)\n",
    "        Xs2_w.append(embs2['representations'][33])\n",
    "    Xs2_w = torch.stack(Xs2_w, dim=0).numpy()\n",
    "    Xs2_w=np.tile(Xs2_w, (no_of_mutations, 1, 1))\n",
    "    \n",
    "    return Xs2_w\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def exctracting_embeddings_esm2_bind(pdb):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    ys = df['bind'].tolist()\n",
    "    no_of_mutations = len(ys)\n",
    "    mutations2_b= [] \n",
    "    Xs2_b = []\n",
    "    for header2, _seq2 in esm.data.read_fasta(FASTA_PATH2_b):\n",
    "        scaled_effect2_b = header2.split('|')[-1]\n",
    "        mutations2_b.append(scaled_effect2_b)\n",
    "        fn = f'{EMB_PATH2_b}/{header2[1:]}.pt'\n",
    "        embs2 = torch.load(fn)\n",
    "        Xs2_b.append(embs2['representations'][33])\n",
    "    Xs2_b = torch.stack(Xs2_b, dim=0).numpy()\n",
    "    Xs2_b=np.tile(Xs2_b, (no_of_mutations, 1, 1))\n",
    "    \n",
    "    return Xs2_b, ys\n",
    "\n",
    "\n",
    "def exctracting_embeddings_1f(pdb):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    ys = df['bind'].tolist()\n",
    "    no_of_mutations = len(ys)\n",
    "    temp= np.load(inverse_path)\n",
    "    inverse= temp['data']\n",
    "    \n",
    "\n",
    "    average_mean_embedding = np.mean(inverse, axis=0)\n",
    "    average_mean_embedding.shape\n",
    "    inverse_mean_reshape = average_mean_embedding.reshape([1, 512])\n",
    "    inverse_mean_reshape.shape\n",
    "    inverse_mean_228=np.tile(inverse_mean_reshape, (no_of_mutations, 1))\n",
    "    \n",
    "\n",
    "    \n",
    "    return inverse_mean_228, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25483b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddg_values = []\n",
    "embeddings = []\n",
    "for pdb in PDBS:\n",
    "    FASTA_PATH = \"/home/PC/Desktop/Protein_language_model/embeddings/{}.fasta\".format(pdb)\n",
    "    EMB_PATH = \"/home/PC/Desktop/Protein_language_model/embeddings/{}_1V\".format(pdb)\n",
    "    FASTA_PATH2 = \"//home/PC/Desktop/Protein_language_model/embeddings/{}.fasta\".format(pdb)\n",
    "    EMB_PATH2 = \"/home/PC/Desktop/Protein_language_model/embeddings/{}_esm2\".format(pdb)\n",
    "    FASTA_PATH_w = \"/home/PC/Desktop/Protein_language_model/embeddings/{}_wild.fasta\".format(pdb)\n",
    "    EMB_PATH_w = \"/home/PC/Desktop/Protein_language_model/embeddings/{}_1V_wild\".format(pdb)\n",
    "    FASTA_PATH2_w = \"/home/PC/Desktop/Protein_language_model/embeddings/{}_wild.fasta\".format(pdb)\n",
    "    EMB_PATH2_w = \"/home/PC/Desktop/Protein_language_model/embeddings/{}_esm2_wild\".format(pdb)\n",
    "    FASTA_PATH_b = \"/home/PC/Desktop/Protein_language_model/embeddings/{}_partner.fasta\".format(pdb)\n",
    "    EMB_PATH_b = \"/home/PC/Desktop/Protein_language_model/embeddings/{}_1V_partner\".format(pdb)\n",
    "    FASTA_PATH2_b = \"/home/PC/Desktop/Protein_language_model/embeddings/{}_partner.fasta\".format(pdb)\n",
    "    EMB_PATH2_b = \"/home/PC/Desktop/Protein_language_model/embeddings/{}_esm2_partner\".format(pdb)\n",
    "    inverse_path = '/home/PC/Desktop/Protein_language_model/embeddings/inverse_{}.npz'.format(pdb)\n",
    "    csv_path = \"/home/PC/Desktop/Protein_language_model/embeddings/{}.csv\".format(pdb)\n",
    "    Xs2= exctracting_embeddings_esm2(pdb)\n",
    "    Xs2_w= exctracting_embeddings_esm2_wild(pdb)\n",
    "    Xs2_b, ys=exctracting_embeddings_esm2_bind(pdb)\n",
    "    inverse, ys=exctracting_embeddings_1f(pdb)\n",
    "    mutant_and_partner_together_esm2 = np.concatenate([Xs2_b, Xs2], axis =1)\n",
    "    \n",
    "    wild_type_and_partner_together_esm2 = np.concatenate([Xs2_b, Xs2_w], axis =1)\n",
    "    mutant_and_partner_together_esm2_mean=np.mean(mutant_and_partner_together_esm2, axis=1)\n",
    "    wild_type_and_partner_together_esm2_mean=np.mean(wild_type_and_partner_together_esm2, axis=1)    \n",
    "    ddg_1v = np.subtract(mutant_and_partner_together_esm2_mean, wild_type_and_partner_together_esm2_mean) \n",
    "    \n",
    "    ddg_esm2_with_inverse = np.concatenate([ddg_1v, inverse], axis =1)\n",
    "    embeddings.append(ddg_esm2_with_inverse)\n",
    "    ddg_values.append(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cd40942",
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_list = [item for sublist in ddg_values for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c8bd773",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_array = embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "596a46c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((152, 1792), (38, 1792), 152, 38)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = 0.8\n",
    "Xs_train, Xs_test, ys_train, ys_test= train_test_split(extracted_array, flattened_list, train_size=train_size, random_state=42)\n",
    "Xs_train.shape, Xs_test.shape, len(ys_train), len(ys_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e360083",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('train.npz', data=Xs_train, label=ys_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5622197f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('test.npz', data=Xs_test, label=ys_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
